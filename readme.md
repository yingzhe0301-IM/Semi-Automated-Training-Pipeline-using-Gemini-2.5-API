# Semiâ€‘AutomatedÂ Object Detection Model TrainingÂ PipelineÂ â€” Using Conversational Image SegmentationÂ withÂ GeminiÂ 2.5Â Flash

![cover](example/cover.png)

Googleâ€™s brandâ€‘new **Conversationalâ€¯Imageâ€¯Segmentation (CIS)** API in *GeminiÂ 2.5Â Flash* lets you describe what you want in plain Englishâ€” _â€œDetect the spill on the desk that needs to be cleanedâ€_, or _â€œFind any construction worker **without a helmet**â€_â€”and immediately receive bounding boxes (or masks).  

This repository shows how to turn those detections into highâ€‘quality training data **and** how to feed that data back into an openâ€‘source detector (YOLO) for *free, lowâ€‘latency inference*.

> For wellâ€‘defined detection tasks, this semiâ€‘automated workflow offers a **costâ€‘effective** and **timeâ€‘saving** path from raw images to a productionâ€‘ready model.

---

## ğŸš€ What Problem Does This Solve?

| Challenge of inference with Gemini           | Our Solution |
|----------------------------------------------|--------------|
| **ğŸ’¸ Cost** â€“ every inference call is billed | Use Gemini to draft annotations, then train YOLO locally |
| **â±ï¸ Latency** â€“ cloud roundâ€‘trip time       | After fineâ€‘tuning YOLO the model runs onâ€‘device in milliseconds |
| **ğŸ¯ Accuracy** â€“ API misses edgeâ€‘cases      | Humanâ€‘inâ€‘theâ€‘loop review in LabelÂ Studio fixes bad boxes |


---

## ğŸ–¼ï¸ Example Queries

<table>
<tr>
<td width="50%"><strong>Complex Scene â€“ â€œFind the spill on the deskâ€</strong><br/><br/><img src="example/spill.png"></td>
<td width="50%"><strong>Safety Compliance â€“ â€œDetect workers <i>with/without</i> helmetsâ€</strong><br/><br/><img src="example/person_with_helmet.PNG"></td>
</tr>
</table>

---

## ğŸ“š Repository Contents

| Script / Folder | Purpose |
|-----------------|---------|
| `gemini.py` | Call the Geminiâ€¯2.5 CIS API, handle auth & retries |
| `converter.py` | Convert Gemini response â†’ LabelÂ Studio `rectanglelabels` JSON |
| `check_gemini_api.py` | Quick sanityâ€‘check of the API & your key |
| `run.ipynb` | Interactive notebook: from images â†’ Gemini â†’ LabelÂ Studio JSON |
| `output_results/` | Raw responses from the API (one *.json* per image) |
| `test_image/` | Sample images to try out the pipeline |

---

## ğŸ”§ Requirements

* **Python**  
* A **Gemini API key** â€“ get one from the [AIÂ Studio console](https://aistudio.google.com/)  
* (Optional) [LabelÂ Studio](https://labelstud.io/) for annotation review

Install the minimal dependencies:

```bash
pip install --upgrade google-genai pillow
```

Set your key (either variable name works):

```bash
export GOOGLE_API_KEY="AIza...your_key..."
# or
export GEMINI_API_KEY="AIza...your_key..."
```

### ğŸŸ Demo: Batch Fish DetectorÂ (GeminiÂ 2.5Â Flash)

This repo started as a tiny proofâ€‘ofâ€‘concept for spotting fish in video frames with Geminiâ€¯2.5â€™s Conversational Image Segmentation.  
If you want a *minimal* example before diving into the full semiâ€‘automated pipeline, try this:

> **Prompt tip** â€“ be explicit about **what** to detect and **what** to return.  
> Example prompt  
> ```text
> Detect all fish in the image and return a JSON list with
> "label" and "box_2d" = [ymin, xmin, ymax, xmax] on a 0â€‘1000 scale.
> ```

Run:

```bash
python gemini.py           # or: jupyter notebook run.ipynb
```

You will get, for each input frame:

* an annotated PNG with green bounding boxes in `output_results/`
* a matching JSON file containing the coordinates

> **Billing note:** each CIS request is billed per call (freeâ€‘trial credits apply).  
> Requesting bounding boxesâ€”as we do hereâ€”is cheaper than fullâ€‘resolution masks.

---

## âš¡ QuickÂ Start

```bash
# 1. Put images into the input folder
mkdir -p test_image
cp ~/my_frames/*.jpg test_image/

# 2. Run the detector (outputs JSON + annotated PNGs)
python gemini.py

# 3. Convert to LabelÂ Studio tasks
python converter.py  # creates tasks.json & import_to_ls_gcs.json

# 4. In LabelÂ Studio
#    â€¢ Import tasks.json (local paths) or import_to_ls_gcs.json (GCS URLs)
#    â€¢ Review / tweak boxes where necessary

# 5. Export corrected dataset and train your favourite YOLO flavour ğŸš€
```

---

## ğŸ—ï¸ Semiâ€‘Automated Training Pipeline

1. **API DraftÂ Annotations** â€“ Gemini detects objects of interest.  
2. **Save to Cloud** â€“ Store frames + detections on GoogleÂ CloudÂ Storage.  
3. **Annotation Review** â€“ Load tasks into LabelÂ Studio, fix mistakes.  
4. **Model Training** â€“ Fineâ€‘tune YOLO (or any detector) on the curated dataset.  
5. **Onâ€‘Device Inference** â€“ Deploy the trained model for zeroâ€‘cost, lowâ€‘latency predictions.

![Annotated sample frame](example/annotated_frame.png)

## ğŸ”— References

* [Conversational Image Segmentation with GeminiÂ 2.5â€¯â€“â€¯Official blog](https://developers.googleblog.com/en/conversational-image-segmentation-gemini-2-5/)
* [LabelÂ Studio Documentation](https://labelstud.io/)
* [YOLO11 Documentation](https://docs.ultralytics.com/models/yolo11/)